{
  "model":{
    "name": "AttentionSeq2Seq",
    "params":{
      "teacher_forcing_ratio": 0.5,
      "device": null
    },
    "encoder":{
      "name": "attention_encoder",
      "params":{
        "hidden_dim": 512,
        "n_layers": 6,
        "n_heads": 8,
        "pf_dim": 2048,
        "dropout": 0.1,
        "device": null
      },
      "optimizer": null
    },
    "decoder":{
      "name": "attention_decoder",
      "params":{
        "hidden_dim": 512,
        "n_layers": 6,
        "n_heads": 8,
        "pf_dim": 2048,
        "dropout": 0.1,
        "device": null
      },
      "optimizer": null
    },
    "embedding":{
      "name": "embedding",
      "params":{
        "vocab_size": 10000,
        "dim": 100
      }
    }
  },
  "training_params":{
    "batch_size": 128,
    "criterion":{
      "name": "CrossEntropyLoss",
      "params":{}
    },
    "optimizer":{
      "name": "NoamOpt",
      "params":{
        "factor": 1,
        "warmup": 2000,
        "optimizer": "Adam",
        "params":{
          "lr": 0,
          "betas": [0.9, 0.98],
          "eps": 1e-9
        }
      }
    }
  },
  "results":{
    "output_dir": "output",
    "model_save_name": "model.ptr"
  }
}